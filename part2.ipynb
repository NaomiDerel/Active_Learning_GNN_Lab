{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T00:21:27.912347400Z",
     "start_time": "2024-07-15T00:21:19.533317100Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "from torch_geometric.nn import SAGEConv\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136140db7336aab",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8991848c218eb533",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T00:25:17.172389200Z",
     "start_time": "2024-07-15T00:25:17.140863300Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_data(nodes_df_path, edges_df_path, subject_mapping_path):\n",
    "    nodes_df = pd.read_csv(nodes_df_path)\n",
    "    edges_df = pd.read_csv(edges_df_path)\n",
    "    with open(subject_mapping_path, 'rb') as f:\n",
    "        subject_mapping = pickle.load(f)\n",
    "    return nodes_df, edges_df, subject_mapping\n",
    "\n",
    "def get_node_id_mapping(nodes_df):\n",
    "    node_id_mapping, inverse_node_id_mapping = dict(), dict()\n",
    "    for i, node_id in enumerate(nodes_df['nodeId']):\n",
    "        node_id_mapping[i] = node_id\n",
    "        inverse_node_id_mapping[node_id] = i\n",
    "    return node_id_mapping, inverse_node_id_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6253a74469198dfa",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_feature_vectors(nodes_df):\n",
    "    # TODO: Complete this function\n",
    "    feature_list = []\n",
    "    for i, feature in enumerate(nodes_df['features']):\n",
    "        feature = list(map(int, feature[1:-1].split(','))) # turn string into list of numbers\n",
    "        feature_list.append(torch.tensor(feature, dtype=torch.float32))\n",
    "    return torch.stack(feature_list)\n",
    "\n",
    "def get_edges(edges_df, inverse_node_id_mapping):\n",
    "    # TODO: Complete this function\n",
    "    edges_list = []\n",
    "    for i, edge in enumerate(edges_df.to_dict('records')):\n",
    "        source_node = inverse_node_id_mapping[edge['sourceNodeId']]\n",
    "        target_node = inverse_node_id_mapping[edge['targetNodeId']]\n",
    "        # relation = edge['relationshipType'] ### the relation is always CITES\n",
    "        edges_list.append(torch.tensor([source_node, target_node], dtype=torch.long))\n",
    "    return torch.stack(edges_list).t().contiguous()\n",
    "\n",
    "def get_labels(nodes_df, subject_mapping):\n",
    "    # TODO: Complete this function\n",
    "    return torch.tensor([subject_mapping[val] for val in nodes_df['subject'].values], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ebc6262a706663",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Graph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2990d42fb3fb06aa",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-15T00:25:18.599753700Z",
     "start_time": "2024-07-15T00:25:18.441126100Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nodes_df_path = 'data/nodes.csv'\n",
    "edges_df_path = 'data/edges.csv'\n",
    "subject_mapping_path = 'data/subject_mapping.pkl'\n",
    "nodes_df, edges_df, subject_mapping = read_data(nodes_df_path, edges_df_path, subject_mapping_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "76e4a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 2708\n",
      "Dimension of features: 1433\n",
      "Number of edges: 10556\n",
      "Number of labels: 7\n",
      "{'Neural_Networks': 0, 'Rule_Learning': 1, 'Reinforcement_Learning': 2, 'Probabilistic_Methods': 3, 'Theory': 4, 'Genetic_Algorithms': 5, 'Case_Based': 6}\n"
     ]
    }
   ],
   "source": [
    "# Check data sizes:\n",
    "print(\"Number of nodes:\", nodes_df.shape[0])\n",
    "print(\"Dimension of features:\", len(nodes_df['features'].iloc[0].split(',')))\n",
    "print(\"Number of edges:\", edges_df.shape[0])\n",
    "print(\"Number of labels:\", len(subject_mapping))\n",
    "print(subject_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "84eb6ec8c176a408",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: torch.Size([2708, 1433])\n",
      "edge_index: torch.Size([2, 10556])\n",
      "y: torch.Size([2708])\n"
     ]
    }
   ],
   "source": [
    "node_id_mapping, inverse_node_id_mapping = get_node_id_mapping(nodes_df)\n",
    "\n",
    "# TODO: These functions need to be implemented. You can decide what are the arguments to these functions.\n",
    "x = get_feature_vectors(nodes_df)\n",
    "print(\"x:\", x.shape)\n",
    "\n",
    "edge_index = get_edges(edges_df, inverse_node_id_mapping)\n",
    "print(\"edge_index:\", edge_index.shape)\n",
    "\n",
    "y = get_labels(nodes_df, subject_mapping)\n",
    "print(\"y:\", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd2655ccbe7fb0d0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('data/indices_dict_part2.pkl', 'rb') as f:\n",
    "    indices_dict = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a0eded454dcceb9",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_mask = torch.tensor([1 if node_id_mapping[i] in indices_dict['train_indices'] else 0 for i in range(x.shape[0])], dtype=torch.bool)\n",
    "valid_mask = torch.tensor([1 if node_id_mapping[i] in indices_dict['valid_indices'] else 0 for i in range(x.shape[0])], dtype=torch.bool)\n",
    "test_mask = torch.tensor([1 if node_id_mapping[i] in indices_dict['test_indices'] else 0 for i in range(x.shape[0])], dtype=torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6d6223faffd4f9c0",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], valid_mask=[2708], test_mask=[2708])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Data(x=x, y=y, edge_index=edge_index, train_mask=train_mask, valid_mask=valid_mask, test_mask=test_mask)\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0a673d4c35320",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# GNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93b3c4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b5ec05507e2191a6",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, output_dim, seed=1):\n",
    "        super().__init__()\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        self.conv1 = SAGEConv((-1, -1), hidden_channels, normalize=True)\n",
    "        self.conv2 = SAGEConv((-1, -1), hidden_channels//2, normalize=True)\n",
    "        self.conv3 = SAGEConv((-1, -1), output_dim, normalize=True)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # TODO: Complete this function\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = F.leaky_relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.conv3(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb36dc1de13f2c",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b0893059e6c29aca",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output_dim = len(subject_mapping)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GraphSAGE(x.shape[1], output_dim).to(device)\n",
    "data = data.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-3)\n",
    "max_epochs = 1000\n",
    "delta_bound = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6aef2870",
   "metadata": {},
   "outputs": [],
   "source": [
    "def early_stop(rel_data_mask):\n",
    "    \"\"\" Compute accuracy on the validation set to check for necessary early stopping. \"\"\"\n",
    "    model.eval()\n",
    "    preds = model(data.x, data.edge_index).argmax(dim=1)\n",
    "    correct = (preds[rel_data_mask] == data.y[rel_data_mask]).sum()\n",
    "    acc = int(correct) / int(rel_data_mask.sum())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af409902fac89678",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def train():\n",
    "    # TODO: Complete this function\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    avg_loss = 0\n",
    "    prev_val_acc = 0\n",
    "    \n",
    "    # train for at most max_epochs:\n",
    "    for epoch in range(1, max_epochs + 1):\n",
    "        # training step:\n",
    "        loss = F.nll_loss(model(data.x, data.edge_index)[train_mask], data.y[train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        avg_loss += loss.item()\n",
    "        \n",
    "        # occasional output and check:\n",
    "        if epoch % 5 == 0:\n",
    "            val_acc = early_stop(valid_mask)\n",
    "            # print('Epoch: {:2d}, Loss: {:.4f}, Accuracy: {:.4f}'.format(epoch, avg_loss / 10, val_acc))\n",
    "            avg_loss = 0\n",
    "\n",
    "            # early stopping (if results are getting worse by at least delta_bound):\n",
    "            if prev_val_acc > val_acc + delta_bound:\n",
    "                break\n",
    "            prev_val_acc = val_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9fa878087d604b8",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5, Loss: 3.0778, Accuracy: 0.6619\n",
      "Epoch: 10, Loss: 3.0682, Accuracy: 0.6639\n",
      "Epoch: 15, Loss: 3.0458, Accuracy: 0.6721\n",
      "Epoch: 20, Loss: 3.0178, Accuracy: 0.7008\n",
      "Epoch: 25, Loss: 3.0210, Accuracy: 0.7213\n",
      "Epoch: 30, Loss: 3.0563, Accuracy: 0.7377\n",
      "Epoch: 35, Loss: 3.0869, Accuracy: 0.7418\n",
      "Epoch: 40, Loss: 3.1025, Accuracy: 0.7398\n",
      "Epoch: 45, Loss: 3.1052, Accuracy: 0.7357\n",
      "Epoch: 50, Loss: 3.0959, Accuracy: 0.7336\n",
      "Epoch: 55, Loss: 3.0728, Accuracy: 0.7254\n",
      "Epoch: 60, Loss: 3.0343, Accuracy: 0.7131\n",
      "Epoch: 65, Loss: 3.0011, Accuracy: 0.7029\n",
      "Epoch: 70, Loss: 3.0260, Accuracy: 0.7008\n",
      "Epoch: 75, Loss: 3.0752, Accuracy: 0.7152\n",
      "Epoch: 80, Loss: 3.0976, Accuracy: 0.7377\n",
      "Epoch: 85, Loss: 3.0955, Accuracy: 0.7439\n",
      "Epoch: 90, Loss: 3.0772, Accuracy: 0.7398\n",
      "Epoch: 95, Loss: 3.0496, Accuracy: 0.7398\n",
      "Epoch: 100, Loss: 3.0272, Accuracy: 0.7357\n",
      "Epoch: 105, Loss: 3.0236, Accuracy: 0.7398\n",
      "Epoch: 110, Loss: 3.0292, Accuracy: 0.7398\n",
      "Epoch: 115, Loss: 3.0341, Accuracy: 0.7398\n",
      "Epoch: 120, Loss: 3.0423, Accuracy: 0.7439\n",
      "Epoch: 125, Loss: 3.0542, Accuracy: 0.7336\n",
      "Epoch: 130, Loss: 3.0636, Accuracy: 0.7213\n",
      "Epoch: 135, Loss: 3.0664, Accuracy: 0.7111\n",
      "Epoch: 140, Loss: 3.0624, Accuracy: 0.7029\n",
      "Epoch: 145, Loss: 3.0562, Accuracy: 0.6988\n",
      "Epoch: 150, Loss: 3.0561, Accuracy: 0.7029\n",
      "Epoch: 155, Loss: 3.0639, Accuracy: 0.7193\n",
      "Epoch: 160, Loss: 3.0735, Accuracy: 0.7336\n",
      "Epoch: 165, Loss: 3.0807, Accuracy: 0.7480\n",
      "Epoch: 170, Loss: 3.0857, Accuracy: 0.7561\n",
      "Epoch: 175, Loss: 3.0889, Accuracy: 0.7643\n",
      "Epoch: 180, Loss: 3.0900, Accuracy: 0.7664\n",
      "Epoch: 185, Loss: 3.0880, Accuracy: 0.7725\n",
      "Epoch: 190, Loss: 3.0822, Accuracy: 0.7787\n",
      "Epoch: 195, Loss: 3.0714, Accuracy: 0.7828\n",
      "Epoch: 200, Loss: 3.0555, Accuracy: 0.7848\n",
      "Epoch: 205, Loss: 3.0363, Accuracy: 0.7848\n",
      "Epoch: 210, Loss: 3.0208, Accuracy: 0.7910\n",
      "Epoch: 215, Loss: 3.0170, Accuracy: 0.7910\n",
      "Epoch: 220, Loss: 3.0240, Accuracy: 0.7889\n",
      "Epoch: 225, Loss: 3.0335, Accuracy: 0.7910\n",
      "Epoch: 230, Loss: 3.0400, Accuracy: 0.7971\n",
      "Epoch: 235, Loss: 3.0426, Accuracy: 0.7930\n",
      "Epoch: 240, Loss: 3.0420, Accuracy: 0.7910\n",
      "Epoch: 245, Loss: 3.0388, Accuracy: 0.7889\n",
      "Epoch: 250, Loss: 3.0331, Accuracy: 0.7910\n",
      "Epoch: 255, Loss: 3.0250, Accuracy: 0.7889\n",
      "Epoch: 260, Loss: 3.0145, Accuracy: 0.7889\n",
      "Epoch: 265, Loss: 3.0024, Accuracy: 0.7930\n",
      "Epoch: 270, Loss: 2.9913, Accuracy: 0.7930\n",
      "Epoch: 275, Loss: 2.9846, Accuracy: 0.7910\n",
      "Epoch: 280, Loss: 2.9842, Accuracy: 0.7930\n",
      "Epoch: 285, Loss: 2.9892, Accuracy: 0.7930\n",
      "Epoch: 290, Loss: 2.9955, Accuracy: 0.7930\n",
      "Epoch: 295, Loss: 3.0002, Accuracy: 0.7951\n",
      "Epoch: 300, Loss: 3.0015, Accuracy: 0.7951\n",
      "Epoch: 305, Loss: 2.9988, Accuracy: 0.8012\n",
      "Epoch: 310, Loss: 2.9923, Accuracy: 0.8033\n",
      "Epoch: 315, Loss: 2.9828, Accuracy: 0.8012\n",
      "Epoch: 320, Loss: 2.9714, Accuracy: 0.8012\n",
      "Epoch: 325, Loss: 2.9599, Accuracy: 0.8012\n",
      "Epoch: 330, Loss: 2.9500, Accuracy: 0.8033\n",
      "Epoch: 335, Loss: 2.9435, Accuracy: 0.8053\n",
      "Epoch: 340, Loss: 2.9414, Accuracy: 0.8053\n",
      "Epoch: 345, Loss: 2.9435, Accuracy: 0.8053\n",
      "Epoch: 350, Loss: 2.9486, Accuracy: 0.8053\n",
      "Epoch: 355, Loss: 2.9545, Accuracy: 0.8053\n",
      "Epoch: 360, Loss: 2.9591, Accuracy: 0.8053\n",
      "Epoch: 365, Loss: 2.9615, Accuracy: 0.8053\n",
      "Epoch: 370, Loss: 2.9607, Accuracy: 0.8053\n",
      "Epoch: 375, Loss: 2.9565, Accuracy: 0.8053\n",
      "Epoch: 380, Loss: 2.9495, Accuracy: 0.8053\n",
      "Epoch: 385, Loss: 2.9403, Accuracy: 0.8053\n",
      "Epoch: 390, Loss: 2.9297, Accuracy: 0.8053\n",
      "Epoch: 395, Loss: 2.9191, Accuracy: 0.8053\n",
      "Epoch: 400, Loss: 2.9108, Accuracy: 0.8074\n",
      "Epoch: 405, Loss: 2.9061, Accuracy: 0.8074\n",
      "Epoch: 410, Loss: 2.9053, Accuracy: 0.8074\n",
      "Epoch: 415, Loss: 2.9074, Accuracy: 0.8074\n",
      "Epoch: 420, Loss: 2.9112, Accuracy: 0.8074\n",
      "Epoch: 425, Loss: 2.9149, Accuracy: 0.8074\n",
      "Epoch: 430, Loss: 2.9172, Accuracy: 0.8074\n",
      "Epoch: 435, Loss: 2.9173, Accuracy: 0.8094\n",
      "Epoch: 440, Loss: 2.9152, Accuracy: 0.8094\n",
      "Epoch: 445, Loss: 2.9114, Accuracy: 0.8094\n",
      "Epoch: 450, Loss: 2.9067, Accuracy: 0.8094\n",
      "Epoch: 455, Loss: 2.9015, Accuracy: 0.8094\n",
      "Epoch: 460, Loss: 2.8960, Accuracy: 0.8094\n",
      "Epoch: 465, Loss: 2.8908, Accuracy: 0.8094\n",
      "Epoch: 470, Loss: 2.8871, Accuracy: 0.8115\n",
      "Epoch: 475, Loss: 2.8851, Accuracy: 0.8115\n",
      "Epoch: 480, Loss: 2.8847, Accuracy: 0.8094\n",
      "Epoch: 485, Loss: 2.8857, Accuracy: 0.8074\n",
      "Epoch: 490, Loss: 2.8875, Accuracy: 0.8053\n",
      "Epoch: 495, Loss: 2.8893, Accuracy: 0.8053\n",
      "Epoch: 500, Loss: 2.8899, Accuracy: 0.8053\n",
      "Epoch: 505, Loss: 2.8890, Accuracy: 0.8053\n",
      "Epoch: 510, Loss: 2.8866, Accuracy: 0.8053\n",
      "Epoch: 515, Loss: 2.8830, Accuracy: 0.8053\n",
      "Epoch: 520, Loss: 2.8786, Accuracy: 0.8053\n",
      "Epoch: 525, Loss: 2.8738, Accuracy: 0.8053\n",
      "Epoch: 530, Loss: 2.8691, Accuracy: 0.8033\n",
      "Epoch: 535, Loss: 2.8653, Accuracy: 0.8033\n",
      "Epoch: 540, Loss: 2.8627, Accuracy: 0.8053\n",
      "Epoch: 545, Loss: 2.8620, Accuracy: 0.8053\n",
      "Epoch: 550, Loss: 2.8629, Accuracy: 0.8053\n",
      "Epoch: 555, Loss: 2.8648, Accuracy: 0.8053\n",
      "Epoch: 560, Loss: 2.8670, Accuracy: 0.8053\n",
      "Epoch: 565, Loss: 2.8688, Accuracy: 0.8074\n",
      "Epoch: 570, Loss: 2.8698, Accuracy: 0.8074\n",
      "Epoch: 575, Loss: 2.8701, Accuracy: 0.8074\n",
      "Epoch: 580, Loss: 2.8695, Accuracy: 0.8094\n",
      "Epoch: 585, Loss: 2.8680, Accuracy: 0.8094\n",
      "Epoch: 590, Loss: 2.8658, Accuracy: 0.8094\n",
      "Epoch: 595, Loss: 2.8627, Accuracy: 0.8115\n",
      "Epoch: 600, Loss: 2.8592, Accuracy: 0.8094\n",
      "Epoch: 605, Loss: 2.8557, Accuracy: 0.8094\n",
      "Epoch: 610, Loss: 2.8527, Accuracy: 0.8094\n",
      "Epoch: 615, Loss: 2.8508, Accuracy: 0.8094\n",
      "Epoch: 620, Loss: 2.8500, Accuracy: 0.8094\n",
      "Epoch: 625, Loss: 2.8503, Accuracy: 0.8094\n",
      "Epoch: 630, Loss: 2.8514, Accuracy: 0.8094\n",
      "Epoch: 635, Loss: 2.8529, Accuracy: 0.8094\n",
      "Epoch: 640, Loss: 2.8545, Accuracy: 0.8094\n",
      "Epoch: 645, Loss: 2.8556, Accuracy: 0.8094\n",
      "Epoch: 650, Loss: 2.8557, Accuracy: 0.8094\n",
      "Epoch: 655, Loss: 2.8549, Accuracy: 0.8094\n",
      "Epoch: 660, Loss: 2.8531, Accuracy: 0.8094\n",
      "Epoch: 665, Loss: 2.8504, Accuracy: 0.8094\n",
      "Epoch: 670, Loss: 2.8471, Accuracy: 0.8094\n",
      "Epoch: 675, Loss: 2.8436, Accuracy: 0.8094\n",
      "Epoch: 680, Loss: 2.8401, Accuracy: 0.8094\n",
      "Epoch: 685, Loss: 2.8372, Accuracy: 0.8094\n",
      "Epoch: 690, Loss: 2.8351, Accuracy: 0.8094\n",
      "Epoch: 695, Loss: 2.8341, Accuracy: 0.8094\n",
      "Epoch: 700, Loss: 2.8343, Accuracy: 0.8094\n",
      "Epoch: 705, Loss: 2.8353, Accuracy: 0.8094\n",
      "Epoch: 710, Loss: 2.8367, Accuracy: 0.8094\n",
      "Epoch: 715, Loss: 2.8380, Accuracy: 0.8074\n",
      "Epoch: 720, Loss: 2.8389, Accuracy: 0.8094\n",
      "Epoch: 725, Loss: 2.8394, Accuracy: 0.8094\n",
      "Epoch: 730, Loss: 2.8392, Accuracy: 0.8094\n",
      "Epoch: 735, Loss: 2.8383, Accuracy: 0.8094\n",
      "Epoch: 740, Loss: 2.8368, Accuracy: 0.8074\n",
      "Epoch: 745, Loss: 2.8349, Accuracy: 0.8074\n",
      "Epoch: 750, Loss: 2.8328, Accuracy: 0.8074\n",
      "Epoch: 755, Loss: 2.8307, Accuracy: 0.8074\n",
      "Epoch: 760, Loss: 2.8287, Accuracy: 0.8074\n",
      "Epoch: 765, Loss: 2.8270, Accuracy: 0.8074\n",
      "Epoch: 770, Loss: 2.8259, Accuracy: 0.8053\n",
      "Epoch: 775, Loss: 2.8253, Accuracy: 0.8053\n",
      "Epoch: 780, Loss: 2.8252, Accuracy: 0.8053\n",
      "Epoch: 785, Loss: 2.8255, Accuracy: 0.8053\n",
      "Epoch: 790, Loss: 2.8260, Accuracy: 0.8053\n",
      "Epoch: 795, Loss: 2.8267, Accuracy: 0.8053\n",
      "Epoch: 800, Loss: 2.8272, Accuracy: 0.8053\n",
      "Epoch: 805, Loss: 2.8274, Accuracy: 0.8053\n",
      "Epoch: 810, Loss: 2.8273, Accuracy: 0.8033\n",
      "Epoch: 815, Loss: 2.8268, Accuracy: 0.8033\n",
      "Epoch: 820, Loss: 2.8259, Accuracy: 0.8033\n",
      "Epoch: 825, Loss: 2.8247, Accuracy: 0.8033\n",
      "Epoch: 830, Loss: 2.8233, Accuracy: 0.8033\n",
      "Epoch: 835, Loss: 2.8218, Accuracy: 0.8033\n",
      "Epoch: 840, Loss: 2.8203, Accuracy: 0.8033\n",
      "Epoch: 845, Loss: 2.8190, Accuracy: 0.8012\n",
      "Epoch: 850, Loss: 2.8179, Accuracy: 0.8012\n",
      "Epoch: 855, Loss: 2.8171, Accuracy: 0.8012\n",
      "Epoch: 860, Loss: 2.8166, Accuracy: 0.8012\n",
      "Epoch: 865, Loss: 2.8163, Accuracy: 0.8012\n",
      "Epoch: 870, Loss: 2.8162, Accuracy: 0.8012\n",
      "Epoch: 875, Loss: 2.8163, Accuracy: 0.8012\n",
      "Epoch: 880, Loss: 2.8163, Accuracy: 0.8012\n",
      "Epoch: 885, Loss: 2.8162, Accuracy: 0.8012\n",
      "Epoch: 890, Loss: 2.8161, Accuracy: 0.8012\n",
      "Epoch: 895, Loss: 2.8157, Accuracy: 0.8012\n",
      "Epoch: 900, Loss: 2.8152, Accuracy: 0.8012\n",
      "Epoch: 905, Loss: 2.8144, Accuracy: 0.8012\n",
      "Epoch: 910, Loss: 2.8136, Accuracy: 0.8012\n",
      "Epoch: 915, Loss: 2.8127, Accuracy: 0.8012\n",
      "Epoch: 920, Loss: 2.8117, Accuracy: 0.8012\n",
      "Epoch: 925, Loss: 2.8108, Accuracy: 0.8012\n",
      "Epoch: 930, Loss: 2.8099, Accuracy: 0.7992\n",
      "Epoch: 935, Loss: 2.8092, Accuracy: 0.7992\n",
      "Epoch: 940, Loss: 2.8087, Accuracy: 0.7992\n",
      "Epoch: 945, Loss: 2.8082, Accuracy: 0.7971\n",
      "Epoch: 950, Loss: 2.8079, Accuracy: 0.7971\n",
      "Epoch: 955, Loss: 2.8077, Accuracy: 0.7951\n",
      "Epoch: 960, Loss: 2.8076, Accuracy: 0.7951\n",
      "Epoch: 965, Loss: 2.8074, Accuracy: 0.7951\n",
      "Epoch: 970, Loss: 2.8071, Accuracy: 0.7951\n",
      "Epoch: 975, Loss: 2.8068, Accuracy: 0.7951\n",
      "Epoch: 980, Loss: 2.8063, Accuracy: 0.7951\n",
      "Epoch: 985, Loss: 2.8058, Accuracy: 0.7951\n",
      "Epoch: 990, Loss: 2.8052, Accuracy: 0.7951\n",
      "Epoch: 995, Loss: 2.8046, Accuracy: 0.7951\n",
      "Epoch: 1000, Loss: 2.8040, Accuracy: 0.7951\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae90da4",
   "metadata": {},
   "source": [
    "Note that we are 'overtraining' in this running case, since 1000 epochs is more than enough and 500 would be a good number to stop at. However, in some runs, when the initial weights are not good, the model might not converge to a good solution in 500 epochs and take much longer to learn.\n",
    "\n",
    "A larger learning rate might help in this case, but significantly ruins the performance afterwards when better accuracies are reached. We also use an early check on the validation set to try and avoid overfitting as much as possible.\n",
    "\n",
    "After trying our some combinations of epochs, learning rate, weights and early stopping, this is the best combination we could find."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0101f5c562bf4b",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa13e475b1ef81a2",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate(rel_data_mask):\n",
    "    model.eval()\n",
    "    preds = model(data.x, data.edge_index).argmax(dim=1)\n",
    "    correct = (preds[rel_data_mask] == data.y[rel_data_mask]).sum()\n",
    "    acc = int(correct) / int(rel_data_mask.sum())\n",
    "    print(f'Accuracy: {acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cef118dafe0b395c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8241\n"
     ]
    }
   ],
   "source": [
    "evaluate(data.test_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
